ðŸ”Š TinyML Acoustic Feature Recognition System (AFRS)This project describes a complete end-to-end system for Acoustic Feature Recognition, leveraging an ESP32 microcontroller, dual-channel I2S for high-fidelity audio capture, a machine learning pipeline (TinyML), and deployment for real-time human-computer interaction (HCI) applications.The core goal is to enable an edge device to autonomously characterize its acoustic environmentâ€”such as room size, occupancy, or material stateâ€”by analyzing its Room Impulse Response (RIR).1. ðŸŽ¤ Data Acquisition & Synchronization (ESP32)This phase focuses on capturing the necessary high-quality acoustic data (the RIR) required to train the machine learning model. This is achieved using a tightly synchronized, dual-I2S, dual-core architecture on the ESP32.1.1 The RIR Measurement TechniqueThe system measures the RIR by broadcasting a known logarithmic sweep (chirp) signal and simultaneously recording the echoes using a PDM microphone. This precise synchronization prevents artifacts and is essential for accurate feature extraction.Stimulus: 20 Hz to 20 kHz Logarithmic Chirp (1 second duration).Capture: 16-bit PCM, 44.1 kHz sample rate.Result: A raw audio file containing the initial chirp followed by its reflections and reverberation.1.2 Dual-Core Architecture and SynchronizationTo ensure zero latency and deterministic timing between playback and recording, the process is split across the ESP32's two cores using FreeRTOS. The synchronization mechanism (semaphore) guarantees that recording begins precisely before the chirp signal starts playing.ComponentFunctionI2S PortCPU CoreRolePlayback TaskGenerates and outputs Chirp via I2S.I2S Port 1 (TX)Core 1 (App CPU)Stimulus generationRecording TaskCaptures PDM microphone data and saves to SD card.I2S Port 0 (RX)Core 0 (Pro CPU)Data capture & storageSynchronizationFreeRTOS Binary SemaphoreN/ABothEnsures recording starts before playback.2. ðŸ§  TinyML Model Training PipelineOnce hundreds of raw RIR files are collected from various environments (e.g., small room, large hall, empty vs. occupied, hard surfaces vs. soft surfaces), they are processed to train a highly efficient deep learning model.2.1 Pre-processing and Feature EngineeringDeconvolution: The raw RIR signal is mathematically deconvolved with the original chirp signal to isolate the true Impulse Response.Acoustic Feature Extraction: The RIR is analyzed to extract standard acoustic features:$T_{60}$ (Reverberation Time)Clarity ($C_{50}, C_{80}$)Definition ($D_{50}$)Spectral features (e.g., Mel-Frequency Cepstral Coefficients - MFCCs) of the impulse response envelope.Labeling: Features are labeled based on the ground truth of the environment (e.g., Acoustic Label: large_conference_room, Occupancy Label: occupied).2.2 Model Selection and OptimizationModel Type: A small, convolutional neural network (CNN) or a recurrent neural network (RNN) is typically used for sequence data like acoustic features.Optimization: Training is performed in Python (TensorFlow/Keras). The resulting model is then converted to TensorFlow Lite (TFLite).Quantization (Crucial for TinyML): The 32-bit floating-point weights are aggressively quantized down to 8-bit integers. This reduces the model size by 75% and dramatically speeds up inference on the ESP32's limited hardware, fitting the model entirely within the device's PSRAM.3. ðŸš€ Deployment and InferenceThe optimized TFLite model is integrated directly into the ESP32 firmware using the TensorFlow Lite Micro library.3.1 On-Device Inference CycleThe ESP32 runs a stripped-down inference cycle: (Playback $\to$ Record $\to$ Deconvolution $\to$ Feature Extraction).The extracted acoustic features are passed to the onboard TFLite Micro model.The model outputs a real-time prediction (e.g., "Medium Room, Unoccupied").Power Management: After a measurement, the device can return to a deep sleep state, optimizing battery life for continuous monitoring.3.2 Hardware Requirements SummaryMicrocontroller: ESP32/ESP32-S3 (with PSRAM required for large buffers and model storage).Input: High-quality PDM microphone (for consistent frequency response).Output: I2S DAC/Amplifier (e.g., MAX98357A) and small speaker (to generate the chirp).Storage: SD Card (for dataset collection and initial setup logging).4. ðŸ’¡ Application and HCI RelevanceThis project shifts acoustic sensing from passive monitoring (like keyword spotting) to active, environmental characterization, opening new avenues for Human-Computer Interaction (HCI).What it DoesThe system provides Contextual Awareness by answering questions about its environment that were previously impossible for a simple microphone:Room State: Is this a large, reflective room or a small, damp office?Change Detection: Has furniture been rearranged or a temporary partition been installed?Occupancy Proxy: Has the reverberation time changed due to new people entering the space?Why it is Useful in HCIAdaptive Audio Interfaces: A voice assistant in a large, echoey room could automatically adjust its output (e.g., lower its gain, reduce bass, speak slower) to maximize intelligibility based on the real-time RIR classification.Smart Building Control: An intelligent building system could determine, based on acoustic analysis, if the space is a gymnasium (high ceilings, long $T_{60}$) versus a library (low $T_{60}$) and adjust HVAC or lighting modes accordingly, without relying solely on visual or motion sensors.Implicit Input: Changes in the room's acoustic profile could be used as an implicit input, such as detecting a rapid change indicating a large object moved or a door slammed, triggering an event without explicit user command.Energy Efficiency: By accurately classifying the room type and state, the system makes localized, fine-grained energy decisions, enhancing the efficiency of smart buildings.
