ðŸ”Š TinyML Acoustic Feature Recognition System (AFRS)This project describes a complete end-to-end system for Acoustic Feature Recognition, leveraging an ESP32 microcontroller, dual-channel I2S for high-fidelity audio capture, a machine learning pipeline (TinyML), and deployment for real-time human-computer interaction (HCI) applications.The core goal is to enable an edge device to autonomously characterize its acoustic environmentâ€”such as room size, occupancy, or material stateâ€”by analyzing its Room Impulse Response (RIR).1. ðŸŽ¤ Data Acquisition & Synchronization (ESP32)This initial phase focuses on capturing the high-quality acoustic data (the RIR) required to train the machine learning model. This is achieved using a tightly synchronized, dual-I2S, dual-core architecture on the ESP32 for high fidelity and zero-latency measurement.1.1 The RIR Measurement TechniqueThe system measures the RIR by broadcasting a known logarithmic sweep (chirp) signal and simultaneously recording the echoes using a PDM microphone. This precise synchronization prevents artifacts and is essential for accurate feature extraction.Stimulus: 20 Hz to 20 kHz Logarithmic Chirp (1 second duration).Capture: 16-bit PCM, 44.1 kHz sample rate.Result: A raw audio file containing the initial chirp followed by its reflections and reverberation.1.2 Dual-Core Architecture and SynchronizationTo ensure deterministic timing and maximum processing bandwidth, the tasks are split across the ESP32's two cores using FreeRTOS. The synchronization mechanism (semaphore) guarantees that recording begins precisely before the chirp signal starts playing.ComponentFunctionI2S PortCPU CoreRolePlayback TaskGenerates and outputs Chirp via I2S.I2S Port 1 (TX)Core 1 (App CPU)Stimulus generationRecording TaskCaptures PDM microphone data and saves to SD card.I2S Port 0 (RX)Core 0 (Pro CPU)Data capture & storageSynchronizationFreeRTOS Binary SemaphoreN/ABothEnsures recording starts before playback.2. ðŸ§  TinyML Model Training PipelineOnce raw RIR files are collected from various environmental conditions (e.g., small room, large hall, empty vs. occupied, hard vs. soft surfaces), they are processed to train a highly efficient deep learning model.2.1 Pre-processing and Feature EngineeringDeconvolution: The raw RIR signal is mathematically deconvolved with the original chirp signal to isolate the true Impulse Response.Acoustic Feature Extraction: The RIR is analyzed to extract standard acoustic features:$T_{60}$ (Reverberation Time)Clarity ($C_{50}, C_{80}$)Definition ($D_{50}$)Spectral features (e.g., Mel-Frequency Cepstral Coefficients - MFCCs) of the impulse response envelope.Labeling: Features are labeled based on the ground truth of the environment (e.g., Acoustic Label: large_conference_room, Occupancy Label: occupied).2.2 Model Selection and OptimizationModel Type: A small, convolutional neural network (CNN) or a recurrent neural network (RNN) is selected for sequence data classification.Optimization: Training is performed in Python (TensorFlow/Keras). The model is then converted to TensorFlow Lite (TFLite).Quantization (Crucial for TinyML): The 32-bit floating-point weights are aggressively quantized down to 8-bit integers. This reduces model size by 75% and dramatically speeds up inference on the ESP32, allowing the model to fit entirely within the device's PSRAM.3. ðŸš€ Deployment and InferenceThe optimized TFLite model is integrated directly into the ESP32 firmware using the TensorFlow Lite Micro library, enabling real-time environmental characterization at the edge.3.1 On-Device Inference CycleThe ESP32 runs a stripped-down measurement cycle: Playback $\to$ Record $\to$ Deconvolution $\to$ Feature Extraction.The extracted acoustic features are passed to the onboard TFLite Micro model.The model outputs a real-time prediction (e.g., "Medium Room, Unoccupied").Power Management: After a measurement, the device can return to a deep sleep state, optimizing battery life for continuous monitoring.3.2 Hardware Requirements SummaryMicrocontroller: ESP32/ESP32-S3 (with PSRAM required).Input: High-quality PDM microphone (for consistent frequency response).Output: I2S DAC/Amplifier (e.g., MAX98357A) and small speaker.Storage: SD Card (for dataset collection and initial setup logging).4. ðŸ’¡ Application and HCI RelevanceThis project shifts acoustic sensing from passive monitoring (like keyword spotting) to active, environmental characterization, opening new avenues for Human-Computer Interaction (HCI).Getty ImagesWhat it Does (Contextual Awareness)The system provides Contextual Awareness by classifying the physical properties of the environment:Room State: Is this a large, reflective room or a small, damp office?Change Detection: Has furniture been rearranged or a temporary partition been installed?Occupancy Proxy: Has the reverberation time changed due to new people entering the space?Why it is Useful in HCIAdaptive Audio Interfaces: A voice assistant in a large, echoey room could automatically adjust its output (e.g., reduce bass, speak slower) to maximize intelligibility based on the real-time RIR classification.Smart Building Control: The system can classify the room type (e.g., gymnasium vs. library) and adjust HVAC or lighting modes accordingly, enhancing building automation efficiency without relying solely on visual or motion sensors.Implicit Input: Changes in the room's acoustic profile could be used as an implicit input, such as detecting a rapid change indicating a large object moved or a door slammed, triggering an event without explicit user command.Energy Efficiency: By accurately classifying the room type and state, the system enables localized, fine-grained energy decisions, enhancing the efficiency of smart buildings.
